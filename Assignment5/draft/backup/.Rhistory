View(df_user)
library(plyr)
df_user = read.csv("D:/#Spring 2023/5580 - Text Mining/Assignment3/userDistinctMilestoneDec151 (1).csv")
df_user2= ddply(df_user,c("id"),function(dfl)paste(dfl$milstone, collapse=","))
df_user$id = NULL
View(df_user2)
View(df_user)
View(df_user2)
df_user2= ddply(df_user,c("id"),function(dfl)paste(dfl$milestone, collapse=","))
library(plyr)
df_user = read.csv("D:/#Spring 2023/5580 - Text Mining/Assignment3/userDistinctMilestoneDec151 (1).csv")
df_user= ddply(df_user,c("id"),function(dfl)paste(dfl$milestone, collapse=","))
df_user$id = NULL
View(df_user)
write.table(df_user,"D:/#Spring 2023/5580 - Text Mining/Assignment3/milestone2.csv",quote=FALSE,row.names=FALSE,col.names=FALSE)
tr = read.transactions("D:/#Spring 2023/5580 - Text Mining/Assignment3/milestone2.csv",format="basket",sep=",")
library("arules")
install.packages("arules")
library("arules")
tr = read.transactions("D:/#Spring 2023/5580 - Text Mining/Assignment3/milestone2.csv",format="basket",sep=",")
rules<- apriori(tr, parameter= list(supp=0.4, conf=0.5))
itemsets=unique(generatingItemsets(rules))
summary(tr)
itemFrequencyPlot(tr, topN=10)
itemFrequencyPlot(tr, topN=20)
itemFrequencyPlot(tr, topN=10)
inspect(rules)
inspect(sort(rules,by="lift"))
inspect(sort(rules,by="lift"))
inspect(sort(rules,by="lift")[1:15])
inspect(sort(rules,by='lift')[1:15])
rules<- apriori(tr, parameter= list(supp=0.3, conf=0.5))
inspect(sort(rules,by='lift')[1:15])
itemsets=unique(generatingItemsets(rules))
inspect(itemsets)
inspect(itemsets,by='support')
inspect(sort(itemsets,by='support'))
inspect(sort(itemsets,by='support')[1:2])
inspect(sort(itemsets,by='support'))
inspect(sort(rules,by='lift')[1:15])
View(itemsets)
inspect(sort(itemsets,by='support'))
library(plyr)
df_user = read.csv("D:/#Spring 2023/5580 - Text Mining/Assignment3/sessionDistinctMilestoneDec15 (2).csv")
View(df_user)
View(df_user)
df_user$key = df_user$id + df_user$date
View(df_user)
df_user$key = df_user$user_id + df_user$date
View(df_user)
df_user$key = df_user$user_id +"_"+ df_user$date
df_user$key = df_user$user_id +"_"+ as.POSIXct(paste(df_user$date), format="%Y-%m-%d")
as.POSIXct(paste(df_user$date), format="%Y-%m-%d")
df_user$key = as.character(df_user$user_id) +"_"+ as.POSIXct(paste(df_user$date), format="%Y-%m-%d")
df_user$key = as.character(df_user$user_id) + as.POSIXct(paste(df_user$date), format="%Y-%m-%d")
df_user$key = paste(as.character(df_user$user_id) , as.POSIXct(df_user$date, format="%Y-%m-%d"))
View(df_user)
library(plyr)
df_session = read.csv("D:/#Spring 2023/5580 - Text Mining/Assignment3/sessionDistinctMilestoneDec15 (2).csv")
df_session$id = paste(as.character(df_session$user_id) , as.POSIXct(df_session$date, format="%Y-%m-%d"))
View(df_session)
View(df_session)
df_session2= ddply(df_session,c("id"),function(dfl)paste(dfl$milestone_name, collapse=","))
View(df_session2)
View(df_session)
View(df_session)
df_session$date = NULL
df_session$user_id = NULL
df_session3= ddply(df_session,c("id"),function(dfl)paste(dfl$milestone_name, collapse=","))
all.equal(df_session2,df_session3)
library(plyr)
df_session = read.csv("D:/#Spring 2023/5580 - Text Mining/Assignment3/sessionDistinctMilestoneDec15 (2).csv")
df_session$id = paste(as.character(df_session$user_id) , as.POSIXct(df_session$date, format="%Y-%m-%d"))
# Transpose source
df_session= ddply(df_session,c("id"),function(dfl)paste(dfl$milestone_name, collapse=","))
# Remove id field
df_session$id = NULL
df_session$date = NULL
df_session$user_id = NULL
# Write to temp file
write.table(df_session,"D:/#Spring 2023/5580 - Text Mining/Assignment3/session2.csv",quote=FALSE,row.names=FALSE,col.names=FALSE)
#install.packages("arules")
library("arules")
# Read temp file
tr = read.transactions("D:/#Spring 2023/5580 - Text Mining/Assignment3/session2.csv",format="basket",sep=",")
# Plot Frequent items
#summary(tr)
itemFrequencyPlot(tr, topN=10)
rules<- apriori(tr, parameter= list(supp=0.3, conf=0.5))
inspect(rules)
summary(tr)
rules<- apriori(tr, parameter= list(supp=0.2, conf=0.5))
inspect(rules)
inspect(sort(rules,by='lift'))
rules<- apriori(tr, parameter= list(supp=0.25, conf=0.5))
inspect(sort(rules,by='lift'))
itemsets=unique(generatingItemsets(rules))
inspect(sort(itemsets,by='support'))
library(arules)
library(reshape2)
library(arulesViz)
user_basket_data <- pivot_wider(user_data,
names_from=milestone,
values_from=milestone,
values_fn=length,
values_fill=0)
install.packages("arulesViz")
user_data <- read.csv("D:/#Spring 2023/5580 - Text Mining/Assignment3/userDistinctMilestoneDec151 (1).csv",
header = TRUE)
# convert data to basket format
user_basket_data <- pivot_wider(user_data,
names_from=milestone,
values_from=milestone,
values_fn=length,
values_fill=0)
user_basket_data <- select(user_basket_data, -1)
library(tidyr)
library(arulesViz)
library(reshape2)
library(dplyr)
user_basket_data <- select(user_basket_data, -1)
user_basket_data <- pivot_wider(user_data,
names_from=milestone,
values_from=milestone,
values_fn=length,
values_fill=0)
user_basket_data <- select(user_basket_data, -1)
for (i in 1:ncol(user_basket_data))
{
user_basket_data[, i] <-
ifelse(user_basket_data[, i] == 1, TRUE, FALSE)
}
View(user_basket_data)
View(user_data)
View(user_basket_data)
View(user_data)
summary(user_basket_data)
association_rules <- apriori(user_basket_data,
parameter= list(supp=0.1,
conf=0.5,
minlen=2,
maxlen=2))
rules_sorted <- sort(association_rules,
by = "confidence",
decreasing = TRUE)
inspect(rules_sorted[1:100])
association_rules2 <- apriori(user_basket_data,
parameter= list(supp=0.3,
conf=0.5))
inspect(sort(association_rules2,by='lift'))
View(user_basket_data)
View(user_basket_data)
View(user_basket_data)
library(plyr)
df_user = read.csv("D:/#Spring 2023/5580 - Text Mining/Assignment3/userDistinctMilestoneDec151 (1).csv")
# Transpose source
df_user= ddply(df_user,c("id"),function(dfl)paste(dfl$milestone, collapse=","))
# Remove id field
df_user$id = NULL
# Write to temp file
write.table(df_user,"D:/#Spring 2023/5580 - Text Mining/Assignment3/milestone2.csv",quote=FALSE,row.names=FALSE,col.names=FALSE)
#install.packages("arules")
library("arules")
# Read temp file
tr = read.transactions("D:/#Spring 2023/5580 - Text Mining/Assignment3/milestone2.csv",format="basket",sep=",")
# Plot Frequent items
#summary(tr)
itemFrequencyPlot(tr, topN=10)
# List the rules
rules<- apriori(tr, parameter= list(supp=0.3, conf=0.5))
inspect(sort(rules,by='lift'))
inspect(sort(association_rules2,by='lift'))
inspect(rules_sorted[1:100])
association_rules2 <- apriori(user_basket_data,
parameter= list(supp=0.1,
conf=0.5,
minlen=2,
maxlen=2))
inspect(sort(association_rules2,by='lift'))
inspect(sort(rules,by='lift'))
itemsets=unique(generatingItemsets(rules))
inspect(sort(itemsets,by='support'))
inspect(sort(rules,by='lift'))
library(plyr)
df_session = read.csv("D:/#Spring 2023/5580 - Text Mining/Assignment3/sessionDistinctMilestoneDec15 (2).csv")
df_session$id = paste(as.character(df_session$user_id) , as.POSIXct(df_session$date, format="%Y-%m-%d"))
# Transpose source
df_session= ddply(df_session,c("id"),function(dfl)paste(dfl$milestone_name, collapse=","))
# Remove id field
df_session$id = NULL
df_session$date = NULL
df_session$user_id = NULL
# Write to temp file
write.table(df_session,"D:/#Spring 2023/5580 - Text Mining/Assignment3/session2.csv",quote=FALSE,row.names=FALSE,col.names=FALSE)
#install.packages("arules")
library("arules")
# Read temp file
tr = read.transactions("D:/#Spring 2023/5580 - Text Mining/Assignment3/session2.csv",format="basket",sep=",")
# Plot Frequent items
#summary(tr)
itemFrequencyPlot(tr, topN=10)
# List the rules
rules<- apriori(tr, parameter= list(supp=0.25, conf=0.5))
inspect(sort(rules,by='lift'))
library(plyr)
df_user = read.csv("D:/#Spring 2023/5580 - Text Mining/Assignment3/userDistinctMilestoneDec151 (1).csv")
# Transpose source
df_user= ddply(df_user,c("id"),function(dfl)paste(dfl$milestone, collapse=","))
# Remove id field
df_user$id = NULL
# Write to temp file
write.table(df_user,"D:/#Spring 2023/5580 - Text Mining/Assignment3/milestone2.csv",quote=FALSE,row.names=FALSE,col.names=FALSE)
#install.packages("arules")
library("arules")
# Read temp file
tr = read.transactions("D:/#Spring 2023/5580 - Text Mining/Assignment3/milestone2.csv",format="basket",sep=",")
# Plot Frequent items
#summary(tr)
itemFrequencyPlot(tr, topN=10)
# List the rules
rules<- apriori(tr, parameter= list(supp=0.3, conf=0.5))
inspect(sort(rules,by='lift'))
library(plyr)
df_user = read.csv("D:/#Spring 2023/5580 - Text Mining/Assignment3/userDistinctMilestoneDec151 (1).csv")
# Transpose source
df_user= ddply(df_user,c("id"),function(dfl)paste(dfl$milestone, collapse=","))
# Remove id field
df_user$id = NULL
# Write to temp file
write.table(df_user,"D:/#Spring 2023/5580 - Text Mining/Assignment3/milestone2.csv",quote=FALSE,row.names=FALSE,col.names=FALSE)
#install.packages("arules")
library("arules")
# Read temp file
tr = read.transactions("D:/#Spring 2023/5580 - Text Mining/Assignment3/milestone2.csv",format="basket",sep=",")
# Plot Frequent items
#summary(tr)
itemFrequencyPlot(tr, topN=10)
# List the rules
rules<- apriori(tr, parameter= list(supp=0.3, conf=0.5))
rules<- apriori(tr, parameter= list(supp=0.4, conf=0.5))
inspect(sort(rules,by='lift'))
# List the rules
rules<- apriori(tr, parameter= list(supp=0.3, conf=0.5))
# List the rules
rules<- apriori(tr, parameter= list(supp=0.4, conf=0.5))
library(plyr)
df_user = read.csv("D:/#Spring 2023/5580 - Text Mining/Assignment3/userDistinctMilestoneDec151 (1).csv")
# Transpose source
df_user= ddply(df_user,c("id"),function(dfl)paste(dfl$milestone, collapse=","))
# Remove id field
df_user$id = NULL
# Write to temp file
write.table(df_user,"D:/#Spring 2023/5580 - Text Mining/Assignment3/milestone2.csv",quote=FALSE,row.names=FALSE,col.names=FALSE)
#install.packages("arules")
library("arules")
# Read temp file
tr = read.transactions("D:/#Spring 2023/5580 - Text Mining/Assignment3/milestone2.csv",format="basket",sep=",")
# Plot Frequent items
#summary(tr)
itemFrequencyPlot(tr, topN=10)
# List the rules
rules<- apriori(tr, parameter= list(supp=0.4, conf=0.5))
library(plyr)
df_user = read.csv("D:/#Spring 2023/5580 - Text Mining/Assignment3/userDistinctMilestoneDec151 (1).csv")
# Transpose source
df_user= ddply(df_user,c("id"),function(dfl)paste(dfl$milestone, collapse=","))
# Remove id field
df_user$id = NULL
# Write to temp file
write.table(df_user,"D:/#Spring 2023/5580 - Text Mining/Assignment3/milestone2.csv",quote=FALSE,row.names=FALSE,col.names=FALSE)
#install.packages("arules")
library("arules")
# Read temp file
tr = read.transactions("D:/#Spring 2023/5580 - Text Mining/Assignment3/milestone2.csv",format="basket",sep=",")
# Plot Frequent items
#summary(tr)
itemFrequencyPlot(tr, topN=10)
# List the rules
rules<- apriori(tr, parameter= list(supp=0.4, conf=0.5))
inspect(sort(rules,by='lift'))
# List item set
itemsets=unique(generatingItemsets(rules))
inspect(sort(itemsets,by='support'))
View(df_user)
summary(tr)
inspect(sort(rules,by='lift')[1:15])
rm(list = ls())    # Clear Environment
cat("\014")        # Clear Console
library(arules)
library(reshape2)
library(arulesViz)
library(Hmisc)
library(tidyr)
library(dplyr)
# read input csv file
user_data <- read.csv("D:/#Spring 2023/5580 - Text Mining/Assignment3/userDistinctMilestoneDec151 (1).csv",
header = TRUE)
# convert data to basket format
user_basket_data <- pivot_wider(user_data,
names_from=milestone,
values_from=milestone,
values_fn=length,
values_fill=0)
user_basket_data <- select(user_basket_data, -1)
# loop over each column and convert 1 to TRUE and 0 to FALSE
for (i in 1:ncol(user_basket_data))
{
user_basket_data[, i] <-
ifelse(user_basket_data[, i] == 1, TRUE, FALSE)
}
summary(user_basket_data)
typeof(user_basket_data)
association_rules <- apriori(user_basket_data,
parameter= list(supp=0.1,
conf=0.5,
minlen=2,
maxlen=2))
rules_sorted <- sort(association_rules,
by = "confidence",
decreasing = TRUE)
plot(rules_sorted[1:20],Â 
inspect(rules_sorted[1:100])
plot(rules_sorted[1:20],
method="graph",
engine="visNetwork")
library(plyr)
library(arules)
df_session = read.csv("D:/#Spring 2023/5580 - Text Mining/Assignment3/sessionDistinctMilestoneDec15 (2).csv")
df_session$id = paste(as.character(df_session$user_id) , as.POSIXct(df_session$date, format="%Y-%m-%d"))
# Transpose source
df_session= ddply(df_session,c("id"),function(dfl)paste(dfl$milestone_name, collapse=","))
# Remove id field
df_session$id = NULL
df_session$date = NULL
df_session$user_id = NULL
# Write to temp file
write.table(df_session,"D:/#Spring 2023/5580 - Text Mining/Assignment3/session2.csv",quote=FALSE,row.names=FALSE,col.names=FALSE)
# Read temp file
tr = read.transactions("D:/#Spring 2023/5580 - Text Mining/Assignment3/session2.csv",format="basket",sep=",")
# Plot Frequent items
#summary(tr)
itemFrequencyPlot(tr, topN=10)
# List the rules
rules<- apriori(tr, parameter= list(supp=0.25, conf=0.5))
inspect(sort(rules,by='lift'))
# Visualize the rules
plot(rules,
method="graph",
engine="visNetwork")
library(arulesViz)
# Visualize the rules
plot(rules,
method="graph",
engine="visNetwork")
library(plyr)
library(arules)
library(arulesViz)
df_session = read.csv("D:/#Spring 2023/5580 - Text Mining/Assignment3/sessionDistinctMilestoneDec15 (2).csv")
df_session$id = paste(as.character(df_session$user_id) , as.POSIXct(df_session$date, format="%Y-%m-%d"))
# Transpose source
df_session= ddply(df_session,c("id"),function(dfl)paste(dfl$milestone_name, collapse=","))
# Remove id field
df_session$id = NULL
df_session$date = NULL
df_session$user_id = NULL
# Write to temp file
write.table(df_session,"D:/#Spring 2023/5580 - Text Mining/Assignment3/session2.csv",quote=FALSE,row.names=FALSE,col.names=FALSE)
# Read temp file
tr = read.transactions("D:/#Spring 2023/5580 - Text Mining/Assignment3/session2.csv",format="basket",sep=",")
# Plot Frequent items
#summary(tr)
itemFrequencyPlot(tr, topN=10)
# List the rules
rules<- apriori(tr, parameter= list(supp=0.25, conf=0.5))
inspect(sort(rules,by='lift'))
# Visualize the rules
plot(rules,
method="graph",
engine="visNetwork")
# List item set
itemsets=unique(generatingItemsets(rules))
inspect(sort(itemsets,by='support'))
itemFrequencyPlot(tr, topN=10)
library(plyr)
library(arules)
library(arulesViz)
df_session = read.csv("D:/#Spring 2023/5580 - Text Mining/Assignment3/sessionDistinctMilestoneDec15 (2).csv")
df_session$id = paste(as.character(df_session$user_id) , as.POSIXct(df_session$date, format="%Y-%m-%d"))
# Transpose source
df_session= ddply(df_session,c("id"),function(dfl)paste(dfl$milestone_name, collapse=","))
# Remove id field
df_session$id = NULL
df_session$date = NULL
df_session$user_id = NULL
# Write to temp file
write.table(df_session,"D:/#Spring 2023/5580 - Text Mining/Assignment3/session2.csv",quote=FALSE,row.names=FALSE,col.names=FALSE)
# Read temp file
tr = read.transactions("D:/#Spring 2023/5580 - Text Mining/Assignment3/session2.csv",format="basket",sep=",")
itemFrequencyPlot(tr, topN=10)
rules<- apriori(tr, parameter= list(supp=0.25, conf=0.5))
inspect(sort(rules,by='lift'))
plot(rules,
method="graph",
engine="visNetwork")
itemFrequencyPlot(tr, topN=10)
plot(rules,
method="graph",
engine="visNetwork")
# Visualize the rules
plot(rules,
method="graph",
engine="visNetwork")
summary(tr)
inspect(sort(rules,by='lift'))
plot(rules,
method="graph",
engine="visNetwork")
itemsets=unique(generatingItemsets(rules))
inspect(sort(itemsets,by='support'))
# Get top 6 product ids in list
top_products <- data %>%
group_by(item_sk) %>%
summarize(total_quantity = sum(quantity)) %>%
select(item_sk, total_quantity) %>%
arrange(desc(total_quantity))
# Get top 6 product ids in list
top_products <- data
library(tidyverse)
library(caret)
setwd("D:\#Spring 2023\5580 - Text Mining\Assignment5\draft")
setwd("D:\\#Spring 2023\\5580 - Text Mining\\Assignment5\\draft")
library(tidyverse)
library(caret)
setwd("D:\\#Spring 2023\\5580 - Text Mining\\Assignment5\\draft")
# Read the data file which has info about top 6 products from sales219
data <- read.csv('data.csv')
# Get top 6 product ids in list
top_products <- data
group_by(item_sk)
library(tidyverse)
library(caret)
setwd("D:/#Spring 2023/5580 - Text Mining/Assignment5/draft")
# Read the data file which has info about top 6 products from sales219
data <- read.csv('data.csv')
# Get top 6 product ids in list
top_products <- data
group_by(item_sk)
library(tidyverse)
library(caret)
setwd("D:/#Spring 2023/5580 - Text Mining/Assignment5/draft")
data <- read.csv('data.csv')
top_products <- data %>%
group_by(item_sk) %>%
summarize(total_quantity = sum(quantity)) %>%
select(item_sk, total_quantity) %>%
arrange(desc(total_quantity))
# Get top 6 product ids in list
top_products <- data %>%
group_by(item_sk) %>%
summarize(total_quantity = sum(quantity)) %>%
select(item_sk, total_quantity) %>%
arrange(desc(total_quantity))
library(tidyverse)
library(caret)
setwd("D:/#Spring 2023/5580 - Text Mining/Assignment5/draft")
# Read the data file which has info about top 6 products from sales219
data <- read.csv('data.csv')
# Get top 6 product ids in list
top_products <- data %>%
group_by(item_sk) %>%
summarize(total_quantity = sum(quantity)) %>%
select(item_sk, total_quantity) %>%
arrange(desc(total_quantity))
# Get the first product and analyze
product <- top_products$item_sk[1]
# Filter the data for selected product only
xy <- data %>%
filter(item_sk == product) %>%
select(-item_sk)
# Check for any missing dates in the data
is_continuous <- all(diff(xy$Date) == 1)
# Remove the data as well
xy$date <- NULL
# Reshape the dataframe to have 7 independent and 1 dependent (8 in total cols)
total_rows = nrow(xy)
rows_to_reshape = total_rows - (total_rows %% 8)
reshaped_xy <-
as.data.frame(matrix(xy[1:rows_to_reshape, 1], ncol = 8, byrow = TRUE))
# Set X and y
X <- reshaped_xy[,1:7]
y <- reshaped_xy[,8]
# Linear Regression
# Using pre-sliced data
myCvControl <- trainControl(method = "repeatedCV",
number=10,
repeats = 5)
# Linear regression
glmFitTime <- train(V8 ~ .,
data = reshaped_xy,
method = "glm",
preProc = c("center", "scale"),
tuneLength = 10,
trControl = myCvControl)
glmFitTime
summary(glmFitTime)
y_hat = predict(glmFitTime, newdata = X)
mean(100*abs(y_hat-y)/y)
#Mean Absolute Percentage Error
# Your error with linear regression
View(user_data)
View(user_data)
View(data)
